{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e2a086-ca51-418f-9200-9f1fc082e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93906db-75c6-47ab-ae64-212cb6a90c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d93866cf0f4fa38ed9e1429004bb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# Загружаем модель и токенизатор, кладем их на GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69cd5fe8-53e9-47f8-9364-afd21ff1baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Как привлечь к себе благосостояние?\",\n",
    "    \"Че делать когда мучает совесть после пьянки?\",\n",
    "    \"Почему вы хотите знать , что будет завтра?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "651df10d-09d1-4764-b9bd-46af1b6f1e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Дай ответ в стиле mail.ru ответов<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Как привлечь к себе благосостояние?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Подавать просто так входы в модель нельзя. Различные instruct-LLM обучены на различных форматах \n",
    "# и нужно обязательно ими пользоваться. Если этого не делать, то качество генераций может как снизиться, так \n",
    "# и стать совсем бессмысленным. Пример форматирования ниже!\n",
    "print(tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"Дай ответ в стиле mail.ru ответов\"}, \n",
    "        {\"role\": \"user\", \"content\": texts[0]}\n",
    "    ], \n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a32a126-d561-483d-949a-cd24c90e68be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----Текст 0-----\n",
      "Как привлечь к себе благосостояние?\n",
      "\n",
      "-----Кандидат 0-----\n",
      "Благосостояние - это чувство комфорта и безопасности, которое можно привлечь в свою жизнь посредством изменений в образе жизни и мышлении.\n",
      "\n",
      "Чтобы привлечь к себе благосостояние, можно попробовать следующие шаги:\n",
      "\n",
      "1. **Измените свою энергетику**: Обращайте внимание на свои мысли, чувства и действия. Когда вы чувствуете себя счастливым, оптимистичным и позитивным, ваша энергия становится привлекательной для благополучия.\n",
      "2. **Практикуйте самообеспечение**: Заботьтесь о своем физическом и эмоциональном здоровье. Включайте в свой распорядок дня упражнения, правильное\n",
      "\n",
      "\n",
      "\n",
      "-----Текст 1-----\n",
      "Че делать когда мучает совесть после пьянки?\n",
      "\n",
      "-----Кандидат 0-----\n",
      "Если мучает совесть после пьянки, то есть несколько вариантов, чтобы решить эту проблему:\n",
      "\n",
      "1. **Признай свои ошибки**: Начни с признания того, что ты совершил ошибку, и извлеки из нее урок. Понимай, что пьянка не решает проблем, а только усугубляет их.\n",
      "\n",
      "2. **Прости себя**: Не стоит себя мучить. Прости себя за то, что ты совершил, и сосредоточься на будущем. Помни, что мы все совершаем ошибки, и главное – извлечь из них пользу.\n",
      "\n",
      "3. **Обратись к друзьям и близким**:\n",
      "\n",
      "\n",
      "\n",
      "-----Текст 2-----\n",
      "Почему вы хотите знать , что будет завтра?\n",
      "\n",
      "-----Кандидат 0-----\n",
      "Это классическая фраза из фильма \"Крестный отец\". В фильме главный герой, Дон Вито Корлеоне, спрашивает своего сына Майкла, зачем тот хочет знать, что будет завтра. Майкл объясняет, что человек, который знает, что будет завтра, уже не нужен сегодня. Итак, ответ прост: знать, что будет завтра, вовсе не обязательно, а часто даже вредно.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "for text_idx, text in  enumerate(texts):\n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"-----Текст {text_idx}-----\")\n",
    "    print(text)\n",
    "    # токенизируем текст, возаращаем тензоры для pytorch\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"Дай ответ в стиле mail.ru ответов\"}, \n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ], \n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    # кладем тензоры на то же устройство (gpu, cpu), что и модель\n",
    "    input_ids = input_ids.to(device) \n",
    "    \n",
    "    # генерируем токены с сэмплингом и температурой, о том, как это работает, можно узнать на курсе!\n",
    "    # также некоторые технические аргументы тоже опустим, но если очень хочется, то обо всех можно почитать в документации\n",
    "    # https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    for i in range(1):\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=torch.ones_like(input_ids),\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            max_length=200,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        # проводим детокенизацию, т.е. превращаем сгенерированные токены обратно в текст\n",
    "        generated_token_ids = outputs[0][input_ids.size(1):]\n",
    "        candidate = tokenizer.decode(generated_token_ids)\n",
    "        print(f\"\\n-----Кандидат {i}-----\")\n",
    "        print(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5191dfb-2cbe-4367-9455-1f21dcfb5533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
