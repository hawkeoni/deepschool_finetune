{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e2a086-ca51-418f-9200-9f1fc082e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93906db-75c6-47ab-ae64-212cb6a90c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da64a616466484085f187b1923aab3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# Загружаем модель и токенизатор, кладем их на GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69cd5fe8-53e9-47f8-9364-afd21ff1baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Как привлечь к себе благосостояние?\",\n",
    "    \"Че делать когда мучает совесть после пьянки?\",\n",
    "    \"Почему вы хотите знать , что будет завтра?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d40b2c-aed0-4913-91cf-2e8ea8e5b225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,    882, 128007,    271, 106523, 108349, 113493,   4929,\n",
       "           7820, 104001, 117934,  23630,  25262,  61390, 101486,     30, 128009,\n",
       "         128006,  78191, 128007,    271]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": texts[0]}], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "651df10d-09d1-4764-b9bd-46af1b6f1e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Дай ответ в стиле mail.ru ответов<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Как привлечь к себе благосостояние?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nКак привлечь к себе благосостояние?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подавать просто так входы в модель нельзя. Различные instruct-LLM обучены на различных форматах \n",
    "# и нужно обязательно ими пользоваться. Если этого не делать, то качество генераций может как снизиться, так \n",
    "# и стать совсем бессмысленным\n",
    "print(tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"Дай ответ в стиле mail.ru ответов\"}, {\"role\": \"user\", \"content\": texts[0]}], tokenize=False))\n",
    "\n",
    "# Вот так выглядит форматирование для llama3.1\n",
    "\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Как привлечь к себе благосостояние?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a32a126-d561-483d-949a-cd24c90e68be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----Текст 0-----\n",
      "Как привлечь к себе благосостояние?\n",
      "\n",
      "-----Кандидат 0-----\n",
      "Чтобы привлечь к себе благосостояние, необходимо поменять свои мысли и настроения. Приносите благодарность за то, что у вас есть, а не жалуитесь на то, что вам не хватает. Также важно научиться ценить и любить себя, а не критиковать и унижать. Это поможет привлечь положительные энергии в вашу жизнь.<|eot_id|>\n",
      "\n",
      "\n",
      "\n",
      "-----Текст 1-----\n",
      "Че делать когда мучает совесть после пьянки?\n",
      "\n",
      "-----Кандидат 0-----\n",
      "Если после пьянки мучает совесть, есть несколько шагов, которые можно предпринять, чтобы решить эту проблему:\n",
      "\n",
      "1. **Принеси в порядок свое жилище**: После выпитого алкоголя может остаться много грязи и беспорядка. Уборка дома поможет вам отвлечься от негативных мыслей и почувствовать себя лучше.\n",
      "\n",
      "2. **Выпей воды**: Пьянка часто приводит к дефициту воды в организме. Пейте много воды, чтобы разбавить алкоголь и снять симптомы похмелья.\n",
      "\n",
      "3. **Немного поспите**: Если возможно, позвольте себе отдохнуть. Похмелье часто бывает более\n",
      "\n",
      "\n",
      "\n",
      "-----Текст 2-----\n",
      "Почему вы хотите знать , что будет завтра?\n",
      "\n",
      "-----Кандидат 0-----\n",
      "Хотелось бы знать, что будет завтра, потому что в жизни так много перемен.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "for text_idx, text in  enumerate(texts):\n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"-----Текст {text_idx}-----\")\n",
    "    print(text)\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"system\", \"content\": \"Дай ответ в стиле mail.ru ответов\"}, \n",
    "         {\"role\": \"user\", \"content\": text}], \n",
    "        return_tensors=\"pt\")  # токенизируем текст, возаращаем тензоры для pytorch\n",
    "    input_ids = input_ids.to(device) # кладем тензоры на то же устройство (gpu, cpu), что и модель\n",
    "    # генерируем токены с сэмплингом и температурой, о том, как это работает, можно узнать на курсе!\n",
    "    # также некоторые технические аргументы тоже опустим, но если очень хочется, то обо всех можно почитать в документации\n",
    "    # https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    for i in range(1):\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            max_length=200,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        # проводим детокенизацию, т.е. превращаем сгенерированные токены обратно в текст\n",
    "        generated_token_ids = outputs[0][input_ids.size(1):]\n",
    "        candidate = tokenizer.decode(generated_token_ids)\n",
    "        print(f\"\\n-----Кандидат {i}-----\")\n",
    "        print(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5191dfb-2cbe-4367-9455-1f21dcfb5533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
